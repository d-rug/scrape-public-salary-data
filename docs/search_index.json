[["index.html", "Scrape public [salary] data Overview", " Scrape public [salary] data Liza Wood 2022-11-01 Overview Data is all over the internet. Some of it is already structured – tables and databases that are already stored online – and much of it is unstructured – information stored on websites, all organized under different formats. This lesson is going to provide an example of web-scraping for each. The project today is learn how to scrape data from public salary database, https://transparentcalifornia.com/. I’m specifically interested in looking for faculty salaries of the department I work in. So, we will be scraping unstructured data – names from a departmental webpage – using regular expressions to clean it up, and then using that to iteratively scrape a structured database of public salaries. "],["scraping-unstructured-data.html", "1 Scraping unstructured data 1.1 Reading in a webpage 1.2 Identifying xpaths", " 1 Scraping unstructured data To scrape salaries from the public salary database, we first need to gather the names of the employees we want to collect salary data on. So let’s go to a UC Davis website and find a departmental page with faculty names. I’ll use the Environmental Science and Policy Department as an example. We should also load in the following libraries that we’re going to use: library(rvest) library(xml2) library(dplyr) 1.1 Reading in a webpage Let’s assign the URL to an object, and use the read_html() function to read the page. url &lt;- &quot;https://desp.ucdavis.edu/faculty&quot; page &lt;- read_html(url) But the page is currently stored as HTML. page ## {html_document} ## &lt;html xmlns=&quot;http://www.w3.org/1999/xhtml&quot; lang=&quot;en&quot; xml:lang=&quot;en&quot;&gt; ## [1] &lt;head profile=&quot;http://www.w3.org/1999/xhtml/vocab&quot;&gt;\\n&lt;meta content=&quot;text/ ... ## [2] &lt;body class=&quot;html not-front not-logged-in one-sidebar sidebar-first page- ... So what do we want out of this mess? We want to find the ‘path’ that defines the location of the things we’re interested in extracting. But to know how to find that path we need to know a little bit about what’s in a web page. This is not my expert area, so I’ll point to a UC Davis DataLab resource on web-scraping that describes elements of webpages and more about HTML and XML. For our purposes, it is important to know that HTML and the related XML use ‘tags’ to mark different parts of the webpage’s data. What we need to identify are the appropriate tags or paths to extract what we want. We can do this by identifying the ‘xpaths’ – the path of tags that highlight particular parts of a page. 1.2 Identifying xpaths I don’t use xpaths everyday, and so I tend to use two methods to help me identify the page’s structure. For very specific paths, I will open up Developer Tools on a Browser using Cmd + Opt + I, then inspecting the elements of the page, Ctrl + F to find the text of what I am looking for in the webpage. Once you find it you can right click &gt; Copy &gt; Xpath. ) When I use this process for the first three names I find the following paths, each on unique their name: * Gwen Arnold: //[@id=\"block-system-main\"]/div/div/div/div[1]/div[2]/h2/a Jesus Barajas: //[@id=\"block-system-main\"]/div/div/div/div[2]/div[2]/h2/a Marissa Baskett: //*[@id=\"block-system-main\"]/div/div/div/div[3]/div[2]/h2/a These are all too specific for trying to take all of the names with one path, but they do help us see the thing in common: All of these names are in the //*[@id=\"block-system-main\"] path, which gives us a sense of what the more generic path might be. We can confirm with the second tool… The InspectorGadget Chrome extension lets you click on blocks of a webpage and identify their Xpaths. When you use this gadget, you can see the more generic path to grab all of the names: //*[(@id = \"block-system-main\")]//a picture of InspectorGadget tool So, we can use this path to pull out the names – turning structured data into data. We pull out the names with the xml_final_all() function, and then pipe that into the html_text() function, which read the text inside the XML nodeset. faculty &lt;- xml_find_all(page, &#39;//*[(@id = &quot;block-system-main&quot;)]//a&#39;) %&gt;% html_text() Now we have faculty names, but it is a bit messy, so we can start to clean it up next: faculty ## [1] &quot;&quot; &quot;Gwen Arnold&quot; &quot;&quot; ## [4] &quot;Jesus M. Barajas&quot; &quot;&quot; &quot;Marissa L. Baskett&quot; ## [7] &quot;&quot; &quot;Howard V. Cornell&quot; &quot;&quot; ## [10] &quot;Xiaoli Dong&quot; &quot;&quot; &quot;Charles R. Goldman&quot; ## [13] &quot;&quot; &quot;Edwin Grosholz&quot; &quot;&quot; ## [16] &quot;Susan L. Handy&quot; &quot;&quot; &quot;Susan P. Harrison&quot; ## [19] &quot;&quot; &quot;Alan Hastings&quot; &quot;&quot; ## [22] &quot;Robert J. Hijmans&quot; &quot;&quot; &quot;Marcel Holyoak&quot; ## [25] &quot;&quot; &quot;Robert A. Johnston&quot; &quot;&quot; ## [28] &quot;John L. Largier&quot; &quot;&quot; &quot;Mark N. Lubell&quot; ## [31] &quot;&quot; &quot;Alan Meier&quot; &quot;&quot; ## [34] &quot;Frances C. Moore&quot; &quot;&quot; &quot;Steven G. Morgan&quot; ## [37] &quot;&quot; &quot;Joan M. Ogden&quot; &quot;&quot; ## [40] &quot;Ben Orlove&quot; &quot;&quot; &quot;James F. Quinn&quot; ## [43] &quot;&quot; &quot;Eliska Rejmankova&quot; &quot;&quot; ## [46] &quot;Peter J. Richerson&quot; &quot;&quot; &quot;Steven Sadro&quot; ## [49] &quot;&quot; &quot;James N. Sanchirico&quot; &quot;&quot; ## [52] &quot;Mark W. Schwartz&quot; &quot;&quot; &quot;Sy I. Schwartz&quot; ## [55] &quot;&quot; &quot;Tyler Scott&quot; &quot;&quot; ## [58] &quot;Andy Sih&quot; &quot;&quot; &quot;Daniel Sperling&quot; ## [61] &quot;&quot; &quot;Michael R. Springborn&quot; &quot;&quot; ## [64] &quot;Thomas P. Tomich&quot; &quot;&quot; &quot;Fernanda S. Valdovinos&quot; "],["formatting-unstructured-data.html", "2 Formatting unstructured data 2.1 What format do we need the data in?", " 2 Formatting unstructured data We have our faculty in a messy vector, so let’s convert this some by turning it into a data frame and filtering out blanks. faculty &lt;- faculty %&gt;% data.frame(&quot;name&quot; = .) %&gt;% filter(name != &quot;&quot;) So, we have the data, but often you need to clean up text data to make it a little more usable. Let’s look at the Transparent California webpage to understand how we want to feed these names into it. 2.1 What format do we need the data in? If you start of Transparent California, you’ll see a plain URL. One way of webscraping across multiple sites is to modify the URL for each page you want to scrape. If we dig into how URLs for pay in the UC system is stored, we find that it looks something like this: https://transparentcalifornia.com/salaries/search/?a=university-of-california&amp;q=Mark+Lubell&amp;y=2021 Let’s look at the pattenr that we want to manipulate: https://transparentcalifornia.com/salaries/search/?a=university-of-california&amp;q=FIRSTNAME+LASTNAME&amp;y=2021 Let’s use some string manipulation tools to extract the first and last names, and paste together those names in within the URL pattern library(stringr) ## Warning: package &#39;stringr&#39; was built under R version 4.1.2 faculty &lt;- faculty %&gt;% # takes first and last word mutate(name_first = word(name, 1), name_last = word(name, -1)) %&gt;% mutate(url_name = paste(name_first, name_last, sep = &quot;+&quot;), url = paste0(&quot;https://transparentcalifornia.com/salaries/search/?a=university-of-california&amp;q=&quot;, url_name, &quot;&amp;y=2021&quot;)) "],["scraping-structured-data-iteravely.html", "3 Scraping structured data iteravely 3.1 Scrape one example 3.2 Iterating scrape across URLs", " 3 Scraping structured data iteravely 3.1 Scrape one example So let’s figure figure out the xpath to scrape one person’s salary data, then we can iterate across multiple names. Let’s follow the same steps we used to scrape a faculty webpage. I know that there is a table in there, so I can use InspectorGadget to help me identify its path. picture of InspectorGadget tool Inspector gadget gave me //*[(@id = \"main-listing\")], which when run in xml_find_all() we see the output is a list with table elements. url &lt;- &quot;https://transparentcalifornia.com/salaries/search/?a=university-of-california&amp;q=Mark+Lubell&amp;y=2021&quot; page &lt;- read_html(url) %&gt;% xml_find_all(&#39;//*[(@id = &quot;main-listing&quot;)]&#39;) page ## {xml_nodeset (1)} ## [1] &lt;table class=&quot;table table-hover table-bordered&quot; id=&quot;main-listing&quot; data-ta ... Since we have a table, now instead of just reading the text we can use rvest’s html_table() function to read the table and wrap it in data.frame() to make it readable. table &lt;- page %&gt;% html_table() %&gt;% data.frame() table ## Name Job.title Regular.pay Overtime.pay ## 1 Mark Lubell Prof-FyUniversity of California, 2021 $198,883.00 $0.00 ## Other.pay Total.pay Benefits Total.pay...benefits ## 1 $18,491.00 $217,374.00 $34,885.00 $252,259.00 3.2 Iterating scrape across URLs Now we the know how to scrape one page, we want to generalize this scraping so that we can input multiple URLs. We’ve wrapped the earlier code into a function with only one argument – the URL. scrape_fx &lt;- function(URL){ url &lt;- URL df &lt;- read_html(url) %&gt;% xml_find_all(&#39;//*[(@id = &quot;main-listing&quot;)]&#39;) %&gt;% html_table() %&gt;% data.frame() return(df) } Now we can apply that function across all of the URLs so that we can compile salary data for all faculty. salary.list &lt;- lapply(faculty$url, scrape_fx) salary.df &lt;- do.call(&quot;rbind&quot;, salary.list) Now we can take a look: DT::datatable(salary.df, rownames = F) "],["cleaning-with-regex.html", "4 Cleaning with regex 4.1 Filter on multiple patterns 4.2 Remove characters", " 4 Cleaning with regex Whenever we’re working with text, from the internet or otherwise, there is going to be some string manipulation that can help us clean and work with the data. To work with text data it is important to have a sense of regular expressions, or regex. My favorite quick resource is the second page of the stringr cheat sheet. Regex is a pattern language that is used for text that can help 4.1 Filter on multiple patterns salary.df &lt;- salary.df %&gt;% select(-Other.pay) %&gt;% filter(str_detect(Job.title, &quot;Prof|Specialist&quot;)) %&gt;% filter(!(str_detect(Job.title, &quot;Adj&quot;))) DT::datatable(salary.df, rownames = F) 4.2 Remove characters For working with the data, however, we want to remove symbols like dollar signs and commas. We can use regular expressions to remove the special characters, but it is important to note that some symbols we use for punctuation have special meanins in regex. For example: * . = every character * $ = end of a string * ^ = end of a string * + = one or more characters When you want to use a symbol in a pattern and use the actual symbol, rather than its special meaning, you need to ‘escape’ the character to signify to the regular expression that you really mean a period (.) rather than every character. So here we can remove dollar signs and commas, but make sure we escape the sign the two backlashes in R. salary.df &lt;- salary.df %&gt;% mutate(Regular.pay = as.numeric(str_remove_all(Regular.pay, &quot;\\\\$|,&quot;)), Total.pay = as.numeric(str_remove_all(Total.pay, &quot;\\\\$|,&quot;))) DT::datatable(salary.df, rownames = F) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
