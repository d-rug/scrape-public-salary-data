[["index.html", "Scrape public [salary] data Overview", " Scrape public [salary] data Liza Wood 2022-11-01 Overview Data is all over the internet. Some of it is already structured – tables and databases that are already stored online – and much of it is unstructured – information stored on websites, all organized under different formats. This lesson is going to provide an example of web-scraping for each. The project today is learn how to scrape data from public salary database, https://transparentcalifornia.com/. I’m specifically interested in looking for faculty salaries of the department I work in. So, we will be scraping unstructured data – names from a departmental webpage – using regular expressions to clean it up, and then using that to iteratively scrape a structured database of public salaries. "],["scraping-unstructured-data.html", "Scraping unstructured data 0.1 Finding your data 0.2 Formatting unstructured data", " Scraping unstructured data 0.1 Finding your data We’ll use the following libraries: library(rvest) library(xml2) library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union url &lt;- &quot;https://desp.ucdavis.edu/faculty&quot; page &lt;- read_html(url) What do we want out of this mess? We want to find the ‘path’ that defines the location that this is written on the path. HTML, xpath I use two main cheats. 1. For very specific paths, I will open up Developer Tools on a Browser, Ctrl+F what I am looking for in the webpage, right click on it, and copy the xpath. [PHOTO] The following paths are here, each on unique their name: * Gwen Arnold: //[@id=\"block-system-main\"]/div/div/div/div[1]/div[2]/h2/a Jesus Barajas: //[@id=\"block-system-main\"]/div/div/div/div[2]/div[2]/h2/a Marissa Baskett: //*[@id=\"block-system-main\"]/div/div/div/div[3]/div[2]/h2/a BUT, then do help us see the thing in common: All of these names are in the //*[@id=\"block-system-main\"] path, which gives us a sense of what the more generic path might be. We can confirm with the second tools.. The Inspector Gadget ChromE extension lets you select blocks of webpage’s and identify their Xpaths. When you use this selector, this allows you to see the more generic path to grab all of the names: //*[(@id = \"block-system-main\")]//a [PHOTO] So, we can use this path to pull out the names – turning structured data into data. We can pull out the names the xml_final_all() function, and then piping that into the html_text() function, which read the text inside the XML nodeset. faculty &lt;- xml_find_all(page, &#39;//*[(@id = &quot;block-system-main&quot;)]//a&#39;) %&gt;% html_text() Now we have faculty names, but there is some blank text in there, so let’s clean this uo faculty &lt;- faculty %&gt;% data.frame(&quot;name&quot; = .) %&gt;% filter(name != &quot;&quot;) faculty ## name ## 1 Gwen Arnold ## 2 Jesus M. Barajas ## 3 Marissa L. Baskett ## 4 Howard V. Cornell ## 5 Xiaoli Dong ## 6 Charles R. Goldman ## 7 Edwin Grosholz ## 8 Susan L. Handy ## 9 Susan P. Harrison ## 10 Alan Hastings ## 11 Robert J. Hijmans ## 12 Marcel Holyoak ## 13 Robert A. Johnston ## 14 John L. Largier ## 15 Mark N. Lubell ## 16 Alan Meier ## 17 Frances C. Moore ## 18 Steven G. Morgan ## 19 Joan M. Ogden ## 20 Ben Orlove ## 21 James F. Quinn ## 22 Eliska Rejmankova ## 23 Peter J. Richerson ## 24 Steven Sadro ## 25 James N. Sanchirico ## 26 Mark W. Schwartz ## 27 Sy I. Schwartz ## 28 Tyler Scott ## 29 Andy Sih ## 30 Daniel Sperling ## 31 Michael R. Springborn ## 32 Thomas P. Tomich ## 33 Fernanda S. Valdovinos So, we have the data, but often you need to clean up text data to make it a little more usable. Let’s look at the Transparent California wepage to understand how we want to feed these names into it. 0.2 Formatting unstructured data 0.2.1 What format do we need the data in? If you start of Transparent California, you’ll see a plain URL. One way of webscraping across multiple sites is to modify the URL for each page you want to scrape. If we dig into how URLs for pay in the UC system is stored, we find that it looks something like this: https://transparentcalifornia.com/salaries/search/?a=university-of-california&amp;q=Mark+Lubell&amp;y=2021 Let’s look at the pattenr that we want to manipulate: https://transparentcalifornia.com/salaries/search/?a=university-of-california&amp;q=FIRSTNAME+LASTNAME&amp;y=2021 0.2.2 Create the URL Let’s use some string manipulation tools to extract the first and last names, and paste together those names in within the URL pattern library(stringr) ## Warning: package &#39;stringr&#39; was built under R version 4.1.2 faculty &lt;- faculty %&gt;% # takes first and last word mutate(name_first = word(name, 1), name_last = word(name, -1)) %&gt;% mutate(url_name = paste(name_first, name_last, sep = &quot;+&quot;), url = paste0(&quot;https://transparentcalifornia.com/salaries/search/?a=university-of-california&amp;q=&quot;, url_name, &quot;&amp;y=2021&quot;)) "],["scraping-structured-data.html", "1 Scraping structured data", " 1 Scraping structured data So let’s figure figure out the xpath to scrape one person’s salary data, then we can iterate across multiple names. Let’s follow the same steps we used to scrape a faculty webpage. I know that there is a table in there, so I can suse Inspector gadget to help me identify its path. Inspector gadget gave me //*[(@id = \"main-listing\")], which reports out the table as a list. Now instead of just reading the text, because this is specified as a table on the webpage itself, we can use rvest’s html_table() function to read the table and wrap it in data.frame() to make it readable. url &lt;- &quot;https://transparentcalifornia.com/salaries/search/?a=university-of-california&amp;q=Mark+Lubell&amp;y=2021&quot; page &lt;- read_html(url) %&gt;% xml_find_all(&#39;//*[(@id = &quot;main-listing&quot;)]&#39;) %&gt;% html_table() %&gt;% data.frame() scrape_fx &lt;- function(URL){ url &lt;- URL df &lt;- read_html(url) %&gt;% xml_find_all(&#39;//*[(@id = &quot;main-listing&quot;)]&#39;) %&gt;% html_table() %&gt;% data.frame() return(df) } salary.list &lt;- lapply(faculty$url, scrape_fx) salary.df &lt;- do.call(&quot;rbind&quot;, salary.list) "],["cleaning-with-regex.html", "Cleaning with regex", " Cleaning with regex "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
